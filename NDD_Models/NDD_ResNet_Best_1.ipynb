{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700636ac-74ec-438f-8cc4-0dfbdf75ce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images loaded: 7590\n",
      "Classes found: ['alzheimers', 'normal', 'parkinsons']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get class names and clean them\n",
    "    classes = sorted([d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))])\n",
    "    classes = [name.replace('_dataset', '') for name in classes]  # Remove '_dataset' from names\n",
    "    \n",
    "    # Create class to index mapping\n",
    "    class_to_idx = {name + '_dataset' if 'dataset' in orig_name else name: idx \n",
    "                   for idx, (name, orig_name) in enumerate(zip(classes, sorted([d for d in os.listdir(dataset_path) \n",
    "                   if os.path.isdir(os.path.join(dataset_path, d))])))}\n",
    "    \n",
    "    # Load images and labels\n",
    "    for orig_class in os.listdir(dataset_path):\n",
    "        if os.path.isdir(os.path.join(dataset_path, orig_class)):\n",
    "            class_path = os.path.join(dataset_path, orig_class)\n",
    "            class_idx = class_to_idx[orig_class]\n",
    "            \n",
    "            for img_name in os.listdir(class_path):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(class_path, img_name)\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    images.append(img)\n",
    "                    labels.append(class_idx)\n",
    "    \n",
    "    return images, labels, classes\n",
    "\n",
    "dataset_path = \"NDD_DATASET\"\n",
    "images, labels, classes = load_dataset(dataset_path)\n",
    "print(f\"Number of images loaded: {len(images)}\")\n",
    "print(f\"Classes found: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf14a1c-ebd9-4f9d-9407-47656921be26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 364x448 -> New size: 360x448\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 260x320 -> New size: 256x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "Original size: 250x320 -> New size: 248x320\n",
      "All images resized to dimensions minimally divisible by 8.\n"
     ]
    }
   ],
   "source": [
    "def resize_images_to_minimally_divisible_by_8(images):\n",
    "    resized_images = []\n",
    "    for img in images:\n",
    "        # Get original dimensions\n",
    "        width, height = img.size\n",
    "        \n",
    "        # Calculate the new dimensions by reducing minimally to make them divisible by 8\n",
    "        new_width = width - (width % 8)\n",
    "        new_height = height - (height % 8)\n",
    "        \n",
    "        # Check if resizing is needed\n",
    "        if width != new_width or height != new_height:\n",
    "            print(f\"Original size: {width}x{height} -> New size: {new_width}x{new_height}\")\n",
    "        \n",
    "        # Crop the image to the new dimensions\n",
    "        resized_img = img.crop((0, 0, new_width, new_height))\n",
    "        resized_images.append(resized_img)\n",
    "    \n",
    "    return resized_images\n",
    "\n",
    "# Resize all images to make their dimensions divisible by 8 and log changes\n",
    "resized_images = resize_images_to_minimally_divisible_by_8(images)\n",
    "\n",
    "# Check that all images are now divisible by 8\n",
    "for idx, img in enumerate(resized_images):\n",
    "    width, height = img.size\n",
    "    assert width % 8 == 0 and height % 8 == 0, f\"Image at index {idx} is not divisible by 8!\"\n",
    "\n",
    "print(\"All images resized to dimensions minimally divisible by 8.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb45f01-4a1c-430b-8e23-8a6a6bb18d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1: Shape = torch.Size([3, 128, 128]), Min = -2.118, Max = 2.588\n",
      "Image 2: Shape = torch.Size([3, 128, 128]), Min = -2.118, Max = 2.640\n",
      "Image 3: Shape = torch.Size([3, 128, 128]), Min = -2.118, Max = 2.274\n",
      "Image 4: Shape = torch.Size([3, 128, 128]), Min = -2.118, Max = 2.640\n",
      "Image 5: Shape = torch.Size([3, 128, 128]), Min = -2.118, Max = 2.623\n",
      "Total tensors created: 7590\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def normalize_and_convert_to_tensor(images):\n",
    "    \"\"\"\n",
    "    Normalize and convert a list of PIL images to PyTorch tensors.\n",
    "    \"\"\"\n",
    "    # Define the transformation: convert to tensor and normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Imagenet dataset.\n",
    "    ])\n",
    "    # Apply the transform to each image\n",
    "    tensors = [transform(img) for img in images]\n",
    "    return tensors\n",
    "\n",
    "# Normalize and convert resized images to tensors\n",
    "tensors = normalize_and_convert_to_tensor(resized_images)\n",
    "\n",
    "# Log some details for verification\n",
    "for i, tensor in enumerate(tensors[:5]):  # Print details of the first 5 tensors\n",
    "    print(f\"Image {i+1}: Shape = {tensor.shape}, Min = {torch.min(tensor):.3f}, Max = {torch.max(tensor):.3f}\")\n",
    "\n",
    "print(f\"Total tensors created: {len(tensors)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c28f53-3920-4bef-b60c-6b3411e30c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Define your transforms here\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Split data into train, validation, and test sets (80:10:10)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    resized_images, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "val_images, test_images, val_labels, test_labels = train_test_split(\n",
    "    test_images, test_labels, test_size=0.5, random_state=42, stratify=test_labels\n",
    ")\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = CustomImageDataset(images=train_images, labels=train_labels, transform=transform)\n",
    "val_dataset = CustomImageDataset(images=val_images, labels=val_labels, transform=transform)\n",
    "test_dataset = CustomImageDataset(images=test_images, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Checking DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)  # Should print batch size and dimensions\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d78233b-f686-4e86-8c1b-8f02bd8733f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dgxa_home/se21uari192/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/dgxa_home/se21uari192/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load ResNet-101 with pretrained weights from ImageNet\n",
    "model = models.resnet101(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer to output 3 classes (Alzheimer’s, Parkinson’s, Normal)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 3)  # 3 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753d96cf-2c29-418e-a235-b0676425c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Loss Function: CrossEntropyLoss (suitable for multi-class classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Adam optimizer (can use other optimizers like SGD as well)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c3a17-450b-4578-95a6-d5bf8597aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 0.2791, Train Accuracy: 0.8725\n",
      "Validation Loss: 0.4617, Validation Accuracy: 0.8024\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 2/50\n",
      "Train Loss: 0.1071, Train Accuracy: 0.9564\n",
      "Validation Loss: 0.1460, Validation Accuracy: 0.9447\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 3/50\n",
      "Train Loss: 0.0420, Train Accuracy: 0.9855\n",
      "Validation Loss: 0.0758, Validation Accuracy: 0.9789\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 4/50\n",
      "Train Loss: 0.0371, Train Accuracy: 0.9872\n",
      "Validation Loss: 0.2930, Validation Accuracy: 0.9289\n",
      "No improvement in validation loss. Early stopping counter: 1/5\n",
      "Epoch 5/50\n",
      "Train Loss: 0.0205, Train Accuracy: 0.9923\n",
      "Validation Loss: 0.0753, Validation Accuracy: 0.9750\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 6/50\n",
      "Train Loss: 0.0287, Train Accuracy: 0.9898\n",
      "Validation Loss: 0.1039, Validation Accuracy: 0.9657\n",
      "No improvement in validation loss. Early stopping counter: 1/5\n",
      "Epoch 7/50\n",
      "Train Loss: 0.0254, Train Accuracy: 0.9908\n",
      "Validation Loss: 0.0518, Validation Accuracy: 0.9802\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 8/50\n",
      "Train Loss: 0.0146, Train Accuracy: 0.9944\n",
      "Validation Loss: 0.0689, Validation Accuracy: 0.9763\n",
      "No improvement in validation loss. Early stopping counter: 1/5\n",
      "Epoch 9/50\n",
      "Train Loss: 0.0180, Train Accuracy: 0.9936\n",
      "Validation Loss: 0.1954, Validation Accuracy: 0.9420\n",
      "No improvement in validation loss. Early stopping counter: 2/5\n",
      "Epoch 10/50\n",
      "Train Loss: 0.0177, Train Accuracy: 0.9932\n",
      "Validation Loss: 0.0939, Validation Accuracy: 0.9684\n",
      "No improvement in validation loss. Early stopping counter: 3/5\n",
      "Epoch 11/50\n",
      "Train Loss: 0.0055, Train Accuracy: 0.9984\n",
      "Validation Loss: 0.1515, Validation Accuracy: 0.9592\n",
      "No improvement in validation loss. Early stopping counter: 4/5\n",
      "Epoch 12/50\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_preds += torch.sum(preds == labels).item()\n",
    "        total_preds += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = correct_preds / total_preds\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Calculate loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_preds += torch.sum(preds == labels).item()\n",
    "            total_preds += labels.size(0)\n",
    "    \n",
    "    eval_loss = running_loss / len(dataloader)\n",
    "    eval_accuracy = correct_preds / total_preds\n",
    "    return eval_loss, eval_accuracy\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs to wait before stopping\n",
    "best_val_loss = float('inf')  # Initialize best validation loss to infinity\n",
    "counter = 0  # Counter for early stopping\n",
    "\n",
    "# Initialize lists to store losses for each epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop with loss tracking and early stopping\n",
    "num_epochs = 50  # Set a high number of epochs to test early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), \"best_Resnet_model.pth\")\n",
    "        print(\"Validation loss improved. Model saved.\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement in validation loss. Early stopping counter: {counter}/{patience}\")\n",
    "        \n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered. Training stopped.\")\n",
    "            break\n",
    "\n",
    "# Plotting the training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a279ce0-5f58-4fed-83c2-1c4a732b6464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate on the test set\n",
    "# model.eval()  # Set to evaluation mode\n",
    "\n",
    "# test_accuracy = 0\n",
    "# test_loss = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in test_loader:\n",
    "#         inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Calculate accuracy\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         correct = (predicted == labels).sum().item()\n",
    "#         test_accuracy += correct\n",
    "#         test_loss += loss.item()\n",
    "\n",
    "#     test_accuracy = 100 * test_accuracy / len(test_loader.dataset)\n",
    "#     print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "# Testing Phase\n",
    "model.eval()\n",
    "correct_test_predictions = 0\n",
    "total_test_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_test_predictions += labels.size(0)\n",
    "        correct_test_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = 100 * correct_test_predictions / total_test_predictions\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba504d-9582-42ff-8ddd-65f886ebf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize containers for predictions and ground truth\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate the model and collect predictions and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Store predictions and labels\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "test_accuracy = 100 * (all_predictions == all_labels).sum() / len(all_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.6f}%\")\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')  # Weighted average for multi-class\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.6f}\")\n",
    "print(f\"Recall: {recall:.6f}\")\n",
    "print(f\"F1-Score: {f1:.6f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=classes))  # Replace `class_names` with your class labels\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d65f4-abc0-48f5-a559-f2ad784f5d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Define class names for the heatmap\n",
    "class_names = ['Alzheimers', 'Normal', 'Parkinsons']  # Replace with your actual class names\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "\n",
    "# Add labels, title, and adjust axis\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165382d-a8e6-4c79-b843-eb9a8916fc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb261a-8d62-4e93-a621-2b859f5e1bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf26d9-3c10-4e3c-bcb4-00eef3ae8055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1277ed-c5d5-4d75-9a1a-2d5afed342a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd88db5-9269-4fea-936c-efbc5cbab9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee09113-f1c1-457a-8735-ca525439b13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080539b-f7a4-4058-939e-3f340d157ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5097334,
     "sourceId": 8534089,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
